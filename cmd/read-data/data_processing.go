package main

import (
	"bufio"
	"encoding/csv"
	"fmt"
	"os"
	"strings"
	"time"
)

// æ•°æ®å¤„ç†ç®¡ç†å™¨
type DataProcessingManager struct {
	scanner *bufio.Scanner
}

// åˆ›å»ºæ•°æ®å¤„ç†ç®¡ç†å™¨
func NewDataProcessingManager() *DataProcessingManager {
	return &DataProcessingManager{
		scanner: bufio.NewScanner(os.Stdin),
	}
}

// é¢„è§ˆæ•°æ®æ˜ å°„æ•ˆæœ
func (dpm *DataProcessingManager) PreviewDataMapping(responseMap map[string]interface{}, data [][]string) {
	fmt.Println("\n" + strings.Repeat("=", 80))
	fmt.Println("                           Data Preview")
	fmt.Println("                   Showing mapped data with current settings")
	fmt.Println(strings.Repeat("=", 80))

	if len(data) == 0 {
		fmt.Println("âŒ No data available for preview")
		return
	}

	processedData := data

	// è·å–æ•°æ®èµ·å§‹è¡Œ
	dataStartRow := getDataStartRow(responseMap)
	if dataStartRow <= 0 || dataStartRow > len(processedData) {
		fmt.Printf("âŒ Invalid data start row: %d (file has %d rows)\n", dataStartRow, len(processedData))
		return
	}

	// æ˜¾ç¤ºæ˜ å°„é…ç½®
	dpm.showMappingConfiguration(responseMap)

	// æ˜¾ç¤ºé¢„è§ˆæ•°æ®
	dpm.showPreviewData(processedData, responseMap, dataStartRow)

	fmt.Println("\n" + strings.Repeat("=", 80))
	fmt.Println("ğŸ“ Preview completed. Press Enter to continue...")
	fmt.Scanln() // ç­‰å¾…ç”¨æˆ·æŒ‰å›è½¦
}

// æ˜¾ç¤ºæ˜ å°„é…ç½®
func (dpm *DataProcessingManager) showMappingConfiguration(responseMap map[string]interface{}) {
	fmt.Println("\nğŸ“‹ Current Field Mapping:")
	fmt.Println(strings.Repeat("-", 60))

	for _, field := range allMappableFields {
		if field == "data_start_row" {
			continue // è·³è¿‡ç³»ç»Ÿé…ç½®å­—æ®µ
		}

		value := getMapValue(responseMap, field)
		displayName := getFieldDisplayName(field)

		if value != "" {
			fmt.Printf("  %-20s -> %s\n", displayName, cleanInferredValue(value))
		} else {
			fmt.Printf("  %-20s -> \n", displayName)
		}
	}

	dataStartRow := getDataStartRow(responseMap)
	fmt.Printf("\nğŸ”§ Data Start Row: %d\n", dataStartRow)
}

// æ˜¾ç¤ºé¢„è§ˆæ•°æ®
func (dpm *DataProcessingManager) showPreviewData(data [][]string, responseMap map[string]interface{}, dataStartRow int) {
	fmt.Println("\nğŸ“Š Data Preview (first 10 rows):")

	// å®šä¹‰ä¸åŒå­—æ®µçš„åˆ—å®½åº¦
	fieldWidths := map[string]int{
		"Date Type":           8,  // WEEKLY
		"Date Code":           12, // 2025-01-06 (å®Œæ•´æ—¥æœŸæ˜¾ç¤º)
		"Geo Type":            8,  // ZIP/DMA/STATE
		"Geo Code":            10, // åœ°ç†ä»£ç 
		"Geo Name":            10, // åœ°ç†åç§°
		"Sales Platform":      10, // å¹³å°åç§°
		"Country Code":        8,  // US/CA
		"Orders":              8,  // è®¢å•æ•°
		"Sales":               10, // é”€å”®é¢
		"Profit":              10, // åˆ©æ¶¦
		"New Customer Orders": 8,  // æ–°å®¢æˆ·è®¢å•
		"New Customer Sales":  10, // æ–°å®¢æˆ·é”€å”®é¢
	}

	// æ˜¾ç¤ºè¡¨å¤´
	headers := []string{}
	for _, field := range allMappableFields {
		if field == "data_start_row" {
			continue
		}
		displayName := getFieldDisplayName(field)
		headers = append(headers, displayName)
	}

	// æ‰“å°è¡¨å¤´
	totalWidth := 0
	for i, header := range headers {
		width := fieldWidths[header]
		if width == 0 {
			width = 10 // é»˜è®¤å®½åº¦
		}
		fmt.Printf("%-*s", width, truncateString(header, width-1))
		if i < len(headers)-1 {
			fmt.Print(" | ")
		}
		totalWidth += width + 3 // 3 for " | "
	}
	fmt.Println()
	fmt.Println(strings.Repeat("-", totalWidth-3))

	// è·å–æ­£ç¡®çš„è¡¨å¤´è¡Œ
	var headerRowIndex int
	if dataStartRow == 1 {
		// å¦‚æœæ•°æ®ä»ç¬¬1è¡Œå¼€å§‹ï¼Œç¬¬1è¡Œæ—¢æ˜¯è¡¨å¤´åˆæ˜¯æ•°æ®èµ·å§‹è¡Œ
		headerRowIndex = 0
	} else {
		// è¡¨å¤´åœ¨æ•°æ®èµ·å§‹è¡Œçš„å‰ä¸€è¡Œ
		headerRowIndex = dataStartRow - 2 // è½¬æ¢ä¸º0åŸºç´¢å¼•
		if headerRowIndex < 0 || headerRowIndex >= len(data) {
			// å¦‚æœè¡¨å¤´è¡Œæ— æ•ˆï¼Œä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
			headerRowIndex = 0
		}
	}
	excelHeaders := data[headerRowIndex]

	// æ˜¾ç¤ºæ•°æ®è¡Œï¼ˆæœ€å¤š10è¡Œï¼‰
	maxRows := 10
	actualDataRows := len(data) - dataStartRow + 1
	if actualDataRows > maxRows {
		actualDataRows = maxRows
	}

	for i := 0; i < actualDataRows && (dataStartRow-1+i) < len(data); i++ {
		row := data[dataStartRow-1+i]
		mappedRow := dpm.mapRowData(row, responseMap, excelHeaders) // ä½¿ç”¨æ­£ç¡®çš„è¡¨å¤´è¡Œ

		for j, value := range mappedRow {
			// ä½¿ç”¨ä¸è¡¨å¤´ç›¸åŒçš„å®½åº¦
			header := headers[j]
			width := fieldWidths[header]
			if width == 0 {
				width = 10 // é»˜è®¤å®½åº¦
			}
			fmt.Printf("%-*s", width, truncateString(value, width-1))
			if j < len(mappedRow)-1 {
				fmt.Print(" | ")
			}
		}
		fmt.Println()
	}

	if len(data)-dataStartRow+1 > maxRows {
		fmt.Printf("\n... and %d more rows\n", len(data)-dataStartRow+1-maxRows)
	}
}

// æ ¹æ®æ˜ å°„é…ç½®è½¬æ¢è¡Œæ•°æ®
func (dpm *DataProcessingManager) mapRowData(row []string, responseMap map[string]interface{}, headers []string) []string {
	result := []string{}

	for _, field := range allMappableFields {
		if field == "data_start_row" {
			continue
		}

		mappedValue := getMapValue(responseMap, field)
		var cellValue string

		if mappedValue == "" {
			cellValue = "" // ç›´æ¥æ˜¾ç¤ºä¸ºç©º
		} else if mappedValue == "VIRTUAL_COUNT" {
			// è™šæ‹Ÿè®¡ç®—å­—æ®µï¼Œæ˜¾ç¤ºä¸º1
			cellValue = "1"
		} else {
			// æŸ¥æ‰¾å¯¹åº”çš„åˆ—ç´¢å¼•
			colIndex := findColumnIndex(headers, cleanInferredValue(mappedValue))
			if colIndex >= 0 && colIndex < len(row) {
				cellValue = row[colIndex]
				// ç©ºå€¼ç›´æ¥æ˜¾ç¤ºä¸ºç©ºå­—ç¬¦ä¸²
			} else {
				// å¦‚æœä¸æ˜¯åˆ—æ˜ å°„ï¼Œå¯èƒ½æ˜¯å›ºå®šå€¼ï¼Œæ¸…ç†æ¨æ–­æ ‡è®°
				cellValue = cleanInferredValue(mappedValue)
			}

			// åº”ç”¨å­—æ®µå¤„ç†æ“ä½œ
			cellValue = applyFieldProcessing(cellValue, field)
		}

		result = append(result, cellValue)
	}

	return result
}

// ä¸‹è½½å®Œæ•´CSVæ–‡ä»¶
func (dpm *DataProcessingManager) DownloadCSV(responseMap map[string]interface{}, filename string) {
	fmt.Println("\n" + strings.Repeat("=", 80))
	fmt.Println("                           CSV Download")
	fmt.Println("                    Generating complete CSV file")
	fmt.Println(strings.Repeat("=", 80))

	// éªŒè¯å¿…å¡«å­—æ®µ
	if !dpm.validateAllRequiredFields(responseMap) {
		fmt.Println("âŒ Cannot download CSV: Required fields are missing")
		fmt.Println("Please complete all required field mappings before downloading.")
		fmt.Println("\nPress Enter to continue...")
		fmt.Scanln()
		return
	}

	// è·å–æ•°æ®èµ·å§‹è¡Œï¼Œç¡®å®šéœ€è¦è¯»å–å¤šå°‘æ•°æ®
	dataStartRow := getDataStartRow(responseMap)

	// è¯»å–å®Œæ•´æ•°æ®ï¼Œéœ€è¦åŒ…å«è¡¨å¤´è¡Œå’Œæ‰€æœ‰æ•°æ®è¡Œ
	fmt.Printf("ğŸ“– Reading complete Excel data (starting from row %d)...\n", dataStartRow)

	// ä½¿ç”¨ReadFileDataè¯»å–æ‰€æœ‰æ•°æ®ï¼ˆä¸é™åˆ¶è¡Œæ•°ï¼‰
	completeData, err := ReadFileData(filename)
	if err != nil {
		fmt.Printf("âŒ Failed to read complete data: %v\n", err)
		fmt.Println("Press Enter to continue...")
		fmt.Scanln()
		return
	}

	if len(completeData) == 0 {
		fmt.Println("âŒ No data found in file")
		fmt.Println("Press Enter to continue...")
		fmt.Scanln()
		return
	}

	// éªŒè¯æ•°æ®èµ·å§‹è¡Œæ˜¯å¦æœ‰æ•ˆ
	if dataStartRow < 1 || dataStartRow > len(completeData) {
		fmt.Printf("âŒ Invalid data start row: %d (file has %d rows)\n", dataStartRow, len(completeData))
		fmt.Println("Please check your data start row setting.")
		fmt.Println("Press Enter to continue...")
		fmt.Scanln()
		return
	}

	processedData := completeData

	// è¯¢é—®ç”¨æˆ·æ˜¯å¦éœ€è¦åˆ†ç»„æ“ä½œ
	needGrouping := dpm.askForGrouping(responseMap)
	var csvContent [][]string

	if needGrouping {
		fmt.Println("ğŸ”„ Processing data with advanced grouping...")
		result, groupErr := dpm.processWithSQLiteGrouping(processedData, responseMap)
		if groupErr != nil {
			fmt.Printf("âŒ Advanced processing failed: %v\n", groupErr)
			fmt.Println("Falling back to normal processing...")
			csvContent = dpm.generateCSVContent(processedData, responseMap)
		} else {
			csvContent = result
		}
	} else {
		fmt.Println("ğŸ”„ Processing data with current mapping...")
		csvContent = dpm.generateCSVContent(processedData, responseMap)
	}

	// ç”Ÿæˆæ–‡ä»¶å
	outputFilename := dpm.generateOutputFilename(filename)

	// å†™å…¥CSVæ–‡ä»¶
	fmt.Printf("ğŸ’¾ Writing CSV file: %s\n", outputFilename)
	err = dpm.writeCSVFile(outputFilename, csvContent)
	if err != nil {
		fmt.Printf("âŒ Failed to write CSV file: %v\n", err)
		fmt.Println("Press Enter to continue...")
		fmt.Scanln()
		return
	}

	// æ˜¾ç¤ºå®Œæˆä¿¡æ¯
	fmt.Println(strings.Repeat("=", 80))
	fmt.Printf("âœ… CSV file generated successfully!\n")
	fmt.Printf("ğŸ“ File location: %s\n", outputFilename)
	fmt.Printf("ğŸ“Š Total rows processed: %d (including header)\n", len(csvContent))
	fmt.Println(strings.Repeat("=", 80))
	fmt.Println("ğŸ‰ Download completed! Program will exit.")
	fmt.Println("Thank you for using the field mapping tool!")

	// ä¸‹è½½å®Œæˆåé€€å‡ºç¨‹åº
	os.Exit(0)
}

// éªŒè¯æ‰€æœ‰å¿…å¡«å­—æ®µæ˜¯å¦å·²å®Œæˆ
func (dpm *DataProcessingManager) validateAllRequiredFields(responseMap map[string]interface{}) bool {
	for fieldName, isRequired := range requered_fields_map {
		if isRequired {
			value := getMapValue(responseMap, fieldName)
			if value == "" {
				return false
			}
		}
	}
	return true
}

// ç”ŸæˆCSVå†…å®¹
func (dpm *DataProcessingManager) generateCSVContent(data [][]string, responseMap map[string]interface{}) [][]string {
	if len(data) == 0 {
		return [][]string{}
	}

	result := [][]string{}

	// ç”ŸæˆCSVè¡¨å¤´
	csvHeader := []string{}
	for _, field := range allMappableFields {
		if field == "data_start_row" {
			continue
		}
		displayName := getFieldDisplayName(field)
		csvHeader = append(csvHeader, displayName)
	}
	result = append(result, csvHeader)

	// è·å–æ•°æ®èµ·å§‹è¡Œ
	dataStartRow := getDataStartRow(responseMap)
	if dataStartRow < 1 || dataStartRow > len(data) {
		return result // åªè¿”å›è¡¨å¤´
	}

	// è¡¨å¤´é€šå¸¸åœ¨æ•°æ®èµ·å§‹è¡Œçš„å‰ä¸€è¡Œ
	var headerRowIndex int
	if dataStartRow == 1 {
		// å¦‚æœæ•°æ®ä»ç¬¬1è¡Œå¼€å§‹ï¼Œç¬¬1è¡Œæ—¢æ˜¯è¡¨å¤´åˆæ˜¯æ•°æ®èµ·å§‹è¡Œ
		headerRowIndex = 0
	} else {
		// è¡¨å¤´åœ¨æ•°æ®èµ·å§‹è¡Œçš„å‰ä¸€è¡Œ
		headerRowIndex = dataStartRow - 2 // è½¬æ¢ä¸º0åŸºç´¢å¼•
		if headerRowIndex < 0 || headerRowIndex >= len(data) {
			// å¦‚æœè¡¨å¤´è¡Œæ— æ•ˆï¼Œä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
			headerRowIndex = 0
		}
	}
	headers := data[headerRowIndex]

	// å¤„ç†æ•°æ®è¡Œï¼ˆä»æ•°æ®èµ·å§‹è¡Œå¼€å§‹ï¼‰
	for i := dataStartRow - 1; i < len(data); i++ {
		row := data[i]
		csvRow := dpm.mapRowToCSV(row, responseMap, headers)
		result = append(result, csvRow)
	}

	return result
}

// å°†Excelè¡Œæ˜ å°„ä¸ºCSVè¡Œ
func (dpm *DataProcessingManager) mapRowToCSV(row []string, responseMap map[string]interface{}, headers []string) []string {
	result := []string{}

	for _, field := range allMappableFields {
		if field == "data_start_row" {
			continue
		}

		mappedValue := getMapValue(responseMap, field)
		var cellValue string

		if mappedValue == "" {
			cellValue = "" // æœªæ˜ å°„çš„å­—æ®µä¸ºç©º
		} else if mappedValue == "VIRTUAL_COUNT" {
			// è™šæ‹Ÿè®¡ç®—å­—æ®µï¼Œè®¾ç½®ä¸º1ï¼ˆæ¯è¡Œä»£è¡¨1ä¸ªè®¢å•ï¼‰
			cellValue = "1"
		} else {
			// æŸ¥æ‰¾å¯¹åº”çš„åˆ—ç´¢å¼•
			cleanMappedValue := cleanInferredValue(mappedValue)
			colIndex := findColumnIndex(headers, cleanMappedValue)
			if colIndex >= 0 && colIndex < len(row) {
				cellValue = row[colIndex]
			} else {
				// å¦‚æœä¸æ˜¯åˆ—æ˜ å°„ï¼Œå¯èƒ½æ˜¯å›ºå®šå€¼
				cellValue = cleanMappedValue
			}

			// åº”ç”¨å­—æ®µå¤„ç†æ“ä½œ
			cellValue = applyFieldProcessing(cellValue, field)
		}

		result = append(result, cellValue)
	}

	return result
}

// ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
func (dpm *DataProcessingManager) generateOutputFilename(inputFilename string) string {
	// è·å–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰
	baseName := inputFilename
	if lastSlash := strings.LastIndex(inputFilename, "/"); lastSlash >= 0 {
		baseName = inputFilename[lastSlash+1:]
	}
	if lastSlash := strings.LastIndex(baseName, "\\"); lastSlash >= 0 {
		baseName = baseName[lastSlash+1:]
	}

	// ç§»é™¤æ‰©å±•å
	if lastDot := strings.LastIndex(baseName, "."); lastDot >= 0 {
		baseName = baseName[:lastDot]
	}

	// æ·»åŠ æ—¶é—´æˆ³å’ŒCSVæ‰©å±•å
	timestamp := fmt.Sprintf("%d", time.Now().Unix())
	return fmt.Sprintf("%s_mapped_%s.csv", baseName, timestamp)
}

// å†™å…¥CSVæ–‡ä»¶
func (dpm *DataProcessingManager) writeCSVFile(filename string, data [][]string) error {
	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	writer := csv.NewWriter(file)
	defer writer.Flush()

	for _, row := range data {
		if err := writer.Write(row); err != nil {
			return err
		}
	}

	return nil
}

// è¯¢é—®ç”¨æˆ·æ˜¯å¦éœ€è¦åˆ†ç»„æ“ä½œ
func (dpm *DataProcessingManager) askForGrouping(responseMap map[string]interface{}) bool {
	fmt.Println("\n" + strings.Repeat("=", 60))
	fmt.Println("                    Data Grouping Option")
	fmt.Println(strings.Repeat("=", 60))

	// æ£€æŸ¥orderså­—æ®µçš„é…ç½®
	ordersValue := getMapValue(responseMap, "orders")
	hasVirtualOrders := ordersValue == "VIRTUAL_COUNT"

	if hasVirtualOrders {
		fmt.Println("\nğŸ” Detected virtual orders calculation in your mapping.")
		fmt.Println("ğŸ“Š Grouping is RECOMMENDED to properly aggregate your data.")
		fmt.Println("")
		fmt.Println("   With grouping enabled:")
		fmt.Println("   â€¢ Your product-level data will be aggregated to order-level")
		fmt.Println("   â€¢ Sales and profit will be summed by dimension groups")
		fmt.Println("   â€¢ Orders will show the actual count per group")
		fmt.Println("")
	} else {
		fmt.Println("\nğŸ“Š Do you want to group/aggregate your data?")
		fmt.Println("   This will group by dimension fields and sum numeric fields.")
		fmt.Println("   Only needed if you have duplicate dimension combinations.")
		fmt.Println("")
	}

	// æ£€æŸ¥æ˜¯å¦é…ç½®äº†order_idå­—æ®µ
	orderIDValue := getMapValue(responseMap, "order_id")
	hasOrderID := orderIDValue != "" && orderIDValue != "VIRTUAL_COUNT"

	fmt.Println("   Grouping configuration:")
	fmt.Println("   â€¢ Group by: date_type, date_code, geo_type, geo_code,")
	fmt.Println("             sales_platform, sales_platform_type, country_code")
	fmt.Println("   â€¢ Sum: sales, profit")
	if hasOrderID {
		fmt.Printf("   â€¢ Count: orders (unique Order IDs from '%s' column)\n", cleanInferredValue(orderIDValue))
		fmt.Println("             ğŸ“Š Using COUNT(DISTINCT order_id) for accurate order counting")
	} else {
		fmt.Println("   â€¢ Count: orders (number of rows in each group)")
		fmt.Println("             ğŸ“Š Using COUNT(*) - each row counts as one order")
	}
	fmt.Println("   â€¢ Keep: All other fields (first non-empty value per group)")
	fmt.Println("")

	if hasVirtualOrders {
		fmt.Print("ğŸ’¡ Enable grouping? (Y/n): ")
	} else {
		fmt.Print("ğŸ’¡ Enable grouping? (y/N): ")
	}

	if !dpm.scanner.Scan() {
		return hasVirtualOrders // é»˜è®¤å€¼åŸºäºæ˜¯å¦æœ‰è™šæ‹Ÿorders
	}

	choice := strings.TrimSpace(strings.ToLower(dpm.scanner.Text()))
	if choice == "" {
		return hasVirtualOrders // ç©ºè¾“å…¥ä½¿ç”¨é»˜è®¤å€¼
	}
	return choice == "y" || choice == "yes"
}

// ä½¿ç”¨SQLiteè¿›è¡Œåˆ†ç»„å¤„ç†
func (dpm *DataProcessingManager) processWithSQLiteGrouping(data [][]string, responseMap map[string]interface{}) ([][]string, error) {
	// åˆ›å»ºSQLiteæœåŠ¡
	sqliteService, err := NewSQLiteService()
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºæ•°æ®å¤„ç†æœåŠ¡å¤±è´¥: %v", err)
	}
	defer sqliteService.Close()

	// å¯¼å…¥æ•°æ®è¿›è¡Œå¤„ç†
	fmt.Println("ğŸ“¥ Importing data for processing...")
	err = sqliteService.ImportData(data, responseMap)
	if err != nil {
		return nil, fmt.Errorf("å¯¼å…¥æ•°æ®å¤±è´¥: %v", err)
	}

	// æ˜¾ç¤ºå¯¼å…¥ç»Ÿè®¡
	rowCount, _ := sqliteService.GetRowCount()
	fmt.Printf("ğŸ“Š Processing %d rows of data\n", rowCount)

	// å®šä¹‰é»˜è®¤çš„åˆ†ç»„å’Œèšåˆå­—æ®µ
	groupByFields := []string{"date_type", "date_code", "geo_type", "geo_code", "sales_platform", "sales_platform_type", "country_code"}
	sumFields := []string{"sales", "profit", "orders"}

	// æ‰§è¡Œåˆ†ç»„æ“ä½œ
	fmt.Println("ğŸ” Executing grouping operation...")
	result, err := sqliteService.ExecuteGroupQuery(groupByFields, sumFields, responseMap)
	if err != nil {
		return nil, fmt.Errorf("æ‰§è¡Œåˆ†ç»„æ“ä½œå¤±è´¥: %v", err)
	}

	fmt.Printf("âœ… Grouping completed: %d groups created\n", len(result)-1)
	return result, nil
}
